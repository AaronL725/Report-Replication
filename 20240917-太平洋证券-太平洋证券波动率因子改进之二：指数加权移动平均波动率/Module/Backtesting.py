"""
å› å­å›æµ‹æ¨¡å—
åŠŸèƒ½ï¼šå¯¹å„ç§æ³¢åŠ¨ç‡å› å­è¿›è¡Œå…¨é¢çš„å›æµ‹åˆ†æ
å›æµ‹è¦æ±‚ï¼š
- æ ·æœ¬ï¼šæ²ªæ·±300æŒ‡æ•°æˆåˆ†è‚¡ä¸ºè‚¡ç¥¨æ± ï¼Œå‰”é™¤åœç‰Œã€STç­‰äº¤æ˜“å¼‚å¸¸è‚¡ç¥¨
- å›æµ‹åŒºé—´ï¼š2021.01.01è‡³2025.04.30
- è°ƒä»“é¢‘ç‡ï¼šæœˆåº¦
- ç»„åˆæƒé‡åˆ†é…ï¼šç­‰æƒ
- å› å­å¤„ç†æ–¹å¼ï¼šå› å­æ–¹å‘è°ƒæ•´ã€ç¼©å°¾è°ƒæ•´ã€å¸‚å€¼è¡Œä¸šä¸­æ€§åŒ–ã€æ ‡å‡†åŒ–
"""

import pandas as pd
import numpy as np
import os
import pickle
import warnings
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime, timedelta
from scipy import stats
from scipy.stats import pearsonr, spearmanr
warnings.filterwarnings('ignore')

# å¯¼å…¥å› å­è®¡ç®—æ¨¡å—
import sys
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'Factor'))

# åŠ¨æ€å¯¼å…¥å› å­æ¨¡å—ä»¥é¿å…å¯¼å…¥é”™è¯¯
def import_factor_modules():
    """åŠ¨æ€å¯¼å…¥å› å­æ¨¡å—"""
    global calculate_ewmavol, calculate_vol_nm, calculate_rankvol, calculate_rvol, calculate_garchvol
    
    try:
        from EWMAVOL import calculate_ewmavol
    except ImportError:
        print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥EWMAVOLæ¨¡å—")
        calculate_ewmavol = None
    
    try:
        from VOL_3M import calculate_vol_nm
    except ImportError:
        print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥VOL_3Mæ¨¡å—")
        calculate_vol_nm = None
    
    try:
        from RANKVOL import calculate_rankvol
    except ImportError:
        print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥RANKVOLæ¨¡å—")
        calculate_rankvol = None
    
    try:
        from RVOL import calculate_rvol
    except ImportError:
        print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥RVOLæ¨¡å—")
        calculate_rvol = None
    
    try:
        from GARCHVOL import calculate_garchvol
    except ImportError:
        print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥GARCHVOLæ¨¡å—")
        calculate_garchvol = None

# è·å–å½“å‰è„šæœ¬ç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(CURRENT_DIR, 'cache')

class FactorBacktester:
    """å› å­å›æµ‹ç±»"""

    def __init__(self, start_date: str = '2021-01-01', end_date: str = '2025-04-30'):
        """
        åˆå§‹åŒ–å›æµ‹å™¨
        
        Args:
            start_date: å›æµ‹å¼€å§‹æ—¥æœŸ
            end_date: å›æµ‹ç»“æŸæ—¥æœŸ
        """
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.data = {}
        self.rebalance_dates = []
        
        # åŠ¨æ€å¯¼å…¥å› å­æ¨¡å—
        import_factor_modules()
          # å› å­é…ç½®
        self.factors_config = {}
        if calculate_ewmavol is not None:
            self.factors_config['EWMAVOL'] = {'func': calculate_ewmavol, 'params': {'window': 60, 'lambda_decay': 0.9}}
        if calculate_vol_nm is not None:
            self.factors_config['VOL_3M'] = {'func': calculate_vol_nm, 'params': {'window': 60}}
        if calculate_rankvol is not None:
            self.factors_config['RANKVOL'] = {'func': calculate_rankvol, 'params': {'window': 60}}
        if calculate_rvol is not None:
            self.factors_config['RVOL'] = {'func': self._calculate_rvol_wrapper, 'params': {'window': 60}}
        if calculate_garchvol is not None:
            self.factors_config['GARCHVOL'] = {'func': self._calculate_garchvol_wrapper, 'params': {'window': 60}}
        
        print(f"å›æµ‹å™¨åˆå§‹åŒ–å®Œæˆ: {start_date} åˆ° {end_date}")
        print(f"å¯ç”¨å› å­: {list(self.factors_config.keys())}")
    def _calculate_rvol_wrapper(self, close_prices: pd.DataFrame, window: int = 60) -> pd.DataFrame:
        """RVOLå› å­è®¡ç®—åŒ…è£…å™¨ï¼Œå¤„ç†FFå› å­å‚æ•°"""
        if 'ff_factors' in self.data:
            return calculate_rvol(close_prices, self.data['ff_factors'], window)
        else:
            print("è­¦å‘Šï¼šç¼ºå°‘Fama-Frenchå› å­æ•°æ®ï¼Œè·³è¿‡RVOLè®¡ç®—")
            return pd.DataFrame(index=close_prices.index, columns=close_prices.columns)
    
    def _calculate_garchvol_wrapper(self, close_prices: pd.DataFrame, window: int = 60) -> pd.DataFrame:
        """GARCHVOLå› å­è®¡ç®—åŒ…è£…å™¨ï¼Œå¤„ç†FFå› å­å‚æ•°"""
        if 'ff_factors' in self.data:
            return calculate_garchvol(close_prices, self.data['ff_factors'], window)
        else:
            print("è­¦å‘Šï¼šç¼ºå°‘Fama-Frenchå› å­æ•°æ®ï¼Œè·³è¿‡GARCHVOLè®¡ç®—")
            return pd.DataFrame(index=close_prices.index, columns=close_prices.columns)
    
    def load_cached_data(self) -> bool:
        """
        åŠ è½½ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½æ‰€æœ‰å¿…è¦æ•°æ®
        """
        print("å¼€å§‹åŠ è½½ç¼“å­˜æ•°æ®...")
        required_files = [
            'processed_close.pkl', 'processed_open.pkl', 'processed_high.pkl', 
            'processed_low.pkl', 'processed_vol.pkl', 'processed_market_cap.pkl',
            'processed_industry.pkl', 'processed_csi300_index_returns.pkl',
            'processed_csi300_constituents.pkl', 'processed_ff_factors.pkl'
        ]
        
        for file_name in required_files:
            file_path = os.path.join(CACHE_DIR, file_name)
            if not os.path.exists(file_path):
                print(f"é”™è¯¯ï¼šç¼“å­˜æ–‡ä»¶ {file_name} ä¸å­˜åœ¨")
                return False
                
            try:
                with open(file_path, 'rb') as f:
                    data_key = file_name.replace('processed_', '').replace('.pkl', '')
                    self.data[data_key] = pickle.load(f)
                print(f"æˆåŠŸåŠ è½½: {file_name}")
            except Exception as e:
                print(f"åŠ è½½ {file_name} æ—¶å‡ºé”™: {e}")
                return False
        
        # è¿‡æ»¤æ•°æ®åˆ°å›æµ‹åŒºé—´
        self._filter_data_by_date()
        print("æ•°æ®åŠ è½½å®Œæˆ")
        return True
    
    def _filter_data_by_date(self):
        """æ ¹æ®å›æµ‹æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®"""
        print(f"è¿‡æ»¤æ•°æ®åˆ°å›æµ‹åŒºé—´: {self.start_date} åˆ° {self.end_date}")
        
        for key, df in self.data.items():
            if isinstance(df, (pd.DataFrame, pd.Series)):
                # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                mask = (df.index >= self.start_date) & (df.index <= self.end_date)
                self.data[key] = df[mask]
                
                if isinstance(df, pd.DataFrame):
                    print(f"{key}: {self.data[key].shape}")
                else:
                    print(f"{key}: {len(self.data[key])} entries")
    
    def _get_trading_days(self) -> pd.DatetimeIndex:
        """è·å–å›æµ‹æœŸé—´çš„äº¤æ˜“æ—¥"""
        return self.data['close'].index
    
    def _get_month_end_dates(self) -> List[pd.Timestamp]:
        """è·å–æœˆæœ«è°ƒä»“æ—¥æœŸ"""
        trading_days = self._get_trading_days()
        
        # è·å–æ¯æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥
        month_ends = []
        for year in range(self.start_date.year, self.end_date.year + 1):
            for month in range(1, 13):
                # æœˆæœ«æ—¥æœŸ
                if year == self.start_date.year and month < self.start_date.month:
                    continue
                if year == self.end_date.year and month > self.end_date.month:
                    break
                
                # æ‰¾è¯¥æœˆçš„æœ€åä¸€ä¸ªäº¤æ˜“æ—¥
                month_start = pd.Timestamp(year, month, 1)
                if month == 12:
                    month_end = pd.Timestamp(year + 1, 1, 1) - pd.Timedelta(days=1)
                else:
                    month_end = pd.Timestamp(year, month + 1, 1) - pd.Timedelta(days=1)
                
                # åœ¨äº¤æ˜“æ—¥ä¸­æ‰¾åˆ°è¯¥æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥
                month_trading_days = trading_days[(trading_days >= month_start) & (trading_days <= month_end)]
                if len(month_trading_days) > 0:
                    month_ends.append(month_trading_days[-1])
        
        self.rebalance_dates = sorted(month_ends)
        print(f"æ‰¾åˆ° {len(self.rebalance_dates)} ä¸ªè°ƒä»“æ—¥æœŸ")
        return self.rebalance_dates
    def _filter_valid_stocks(self, factor_data: pd.DataFrame, date: pd.Timestamp) -> pd.DataFrame:
        """
        è¿‡æ»¤æœ‰æ•ˆè‚¡ç¥¨ï¼Œåªä¿ç•™CSI300æˆåˆ†è‚¡ï¼Œå¹¶å‰”é™¤åœç‰Œã€STç­‰å¼‚å¸¸è‚¡ç¥¨
        
        Args:
            factor_data: å› å­æ•°æ®
            date: å½“å‰æ—¥æœŸ
            
        Returns:
            è¿‡æ»¤åçš„å› å­æ•°æ®
        """
        if date not in factor_data.index:
            return pd.DataFrame()
        
        factor_values = factor_data.loc[date].dropna()
        
        # 1. é¦–å…ˆè¿‡æ»¤CSI300æˆåˆ†è‚¡
        if 'csi300_constituents' in self.data and date in self.data['csi300_constituents'].index:
            csi300_constituents = self.data['csi300_constituents'].loc[date]
            # è·å–å½“æ—¥çš„CSI300æˆåˆ†è‚¡ï¼ˆå€¼ä¸ºTrueçš„è‚¡ç¥¨ï¼‰
            csi300_stocks = csi300_constituents[csi300_constituents == True].index
            # åªä¿ç•™CSI300æˆåˆ†è‚¡
            factor_values = factor_values[factor_values.index.intersection(csi300_stocks)]
            print(f"æ—¥æœŸ {date}: CSI300æˆåˆ†è‚¡è¿‡æ»¤åå‰©ä½™ {len(factor_values)} åªè‚¡ç¥¨")
        else:
            print(f"è­¦å‘Šï¼šæ—¥æœŸ {date} æ— CSI300æˆåˆ†è‚¡æ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨")
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰ä»·æ ¼æ•°æ®
        if date in self.data['close'].index:
            close_prices = self.data['close'].loc[date]
            # åªä¿ç•™æœ‰æ”¶ç›˜ä»·çš„è‚¡ç¥¨
            valid_stocks = factor_values.index.intersection(close_prices.dropna().index)
            factor_values = factor_values[valid_stocks]
        
        # 3. æ£€æŸ¥æˆäº¤é‡ï¼Œè¿‡æ»¤åœç‰Œè‚¡ç¥¨
        if date in self.data['vol'].index:
            volumes = self.data['vol'].loc[date]
            # åªä¿ç•™æœ‰æˆäº¤é‡çš„è‚¡ç¥¨
            trading_stocks = volumes[volumes > 0].index
            factor_values = factor_values[factor_values.index.intersection(trading_stocks)]
        
        # 4. ç®€å•è¿‡æ»¤STè‚¡ç¥¨ï¼ˆåŸºäºè‚¡ç¥¨ä»£ç åŒ…å«STæ ‡è¯†ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´ç²¾ç¡®çš„STè‚¡ç¥¨è¯†åˆ«
        non_st_stocks = [stock for stock in factor_values.index 
                        if 'ST' not in str(stock).upper()]
        factor_values = factor_values[non_st_stocks]
        
        return factor_values
    
    def _neutralize_factor(self, factor_values: pd.Series, date: pd.Timestamp) -> pd.Series:
        """
        å› å­ä¸­æ€§åŒ–å¤„ç†ï¼šå¸‚å€¼å’Œè¡Œä¸šä¸­æ€§åŒ–
        
        Args:
            factor_values: åŸå§‹å› å­å€¼
            date: å½“å‰æ—¥æœŸ
            
        Returns:
            ä¸­æ€§åŒ–åçš„å› å­å€¼
        """
        if len(factor_values) == 0:
            return factor_values
          # è·å–å¸‚å€¼æ•°æ®
        if date not in self.data['market_cap'].index:
            print(f"è­¦å‘Šï¼šæ—¥æœŸ {date} æ— å¸‚å€¼æ•°æ®ï¼Œè·³è¿‡å¸‚å€¼ä¸­æ€§åŒ–")
            market_cap = None
        else:
            market_cap = self.data['market_cap'].loc[date]
            # åªä¿ç•™ä¸å› å­å€¼æœ‰äº¤é›†çš„è‚¡ç¥¨
            common_stocks_mc = factor_values.index.intersection(market_cap.index)
            market_cap = market_cap[common_stocks_mc].dropna()
        
        # è·å–è¡Œä¸šæ•°æ®
        if date not in self.data['industry'].index:
            print(f"è­¦å‘Šï¼šæ—¥æœŸ {date} æ— è¡Œä¸šæ•°æ®ï¼Œè·³è¿‡è¡Œä¸šä¸­æ€§åŒ–")
            industry_data = None
        else:
            industry_data = self.data['industry'].loc[date]
            # åªä¿ç•™ä¸å› å­å€¼æœ‰äº¤é›†çš„è‚¡ç¥¨
            common_stocks_ind = factor_values.index.intersection(industry_data.index)
            industry_data = industry_data[common_stocks_ind].dropna()
        
        # ç¡®ä¿å› å­å€¼ã€å¸‚å€¼ã€è¡Œä¸šæ•°æ®çš„è‚¡ç¥¨ä¸€è‡´
        if market_cap is not None and industry_data is not None:
            common_stocks = factor_values.index.intersection(market_cap.index).intersection(industry_data.index)
        elif market_cap is not None:
            common_stocks = factor_values.index.intersection(market_cap.index)
        elif industry_data is not None:
            common_stocks = factor_values.index.intersection(industry_data.index)
        else:
            return factor_values  # æ²¡æœ‰ä¸­æ€§åŒ–æ•°æ®ï¼Œè¿”å›åŸå€¼
        
        if len(common_stocks) < 10:
            print(f"è­¦å‘Šï¼šæ—¥æœŸ {date} æœ‰æ•ˆè‚¡ç¥¨æ•°é‡ä¸è¶³ ({len(common_stocks)})ï¼Œè·³è¿‡ä¸­æ€§åŒ–")
            return factor_values
        
        factor_values = factor_values[common_stocks]
        
        # å‡†å¤‡å›å½’æ•°æ®
        y = factor_values.values
        X = []
        
        # æ·»åŠ å¸‚å€¼å› å­
        if market_cap is not None:
            log_market_cap = np.log(market_cap[common_stocks].values)
            X.append(log_market_cap)
        
        # æ·»åŠ è¡Œä¸šå“‘å˜é‡
        if industry_data is not None:
            industries = industry_data[common_stocks]
            unique_industries = industries.unique()
            
            for industry in unique_industries[1:]:  # æ’é™¤ç¬¬ä¸€ä¸ªè¡Œä¸šä½œä¸ºåŸºå‡†
                industry_dummy = (industries == industry).astype(int).values
                X.append(industry_dummy)
        
        if len(X) == 0:
            return factor_values
        
        X = np.column_stack(X)
        X = np.column_stack([np.ones(len(X)), X])  # æ·»åŠ å¸¸æ•°é¡¹
        
        try:
            # å›å½’ä¸­æ€§åŒ–
            beta = np.linalg.lstsq(X, y, rcond=None)[0]
            y_pred = X @ beta
            residuals = y - y_pred
            
            # è¿”å›æ®‹å·®ä½œä¸ºä¸­æ€§åŒ–åçš„å› å­å€¼
            return pd.Series(residuals, index=common_stocks)
        except np.linalg.LinAlgError:
            print(f"è­¦å‘Šï¼šæ—¥æœŸ {date} å›å½’å¤±è´¥ï¼Œè¿”å›åŸå› å­å€¼")
            return factor_values
    
    def _standardize_factor(self, factor_values: pd.Series) -> pd.Series:
        """
        å› å­æ ‡å‡†åŒ–ï¼šå»å‡å€¼ã€é™¤æ ‡å‡†å·®
        
        Args:
            factor_values: å› å­å€¼
            
        Returns:
            æ ‡å‡†åŒ–åçš„å› å­å€¼
        """
        if len(factor_values) == 0:
            return factor_values
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        mean_val = factor_values.mean()
        std_val = factor_values.std()
        
        if std_val == 0 or np.isnan(std_val):
            print("è­¦å‘Šï¼šå› å­æ ‡å‡†å·®ä¸º0æˆ–NaNï¼Œè·³è¿‡æ ‡å‡†åŒ–")
            return factor_values
        
        # æ ‡å‡†åŒ–
        standardized = (factor_values - mean_val) / std_val
        return standardized
    
    def _winsorize_factor(self, factor_values: pd.Series, quantile: float = 0.01) -> pd.Series:
        """
        å› å­ç¼©å°¾å¤„ç†
        
        Args:
            factor_values: å› å­å€¼
            quantile: ç¼©å°¾æ¯”ä¾‹ï¼Œé»˜è®¤1%
            
        Returns:
            ç¼©å°¾åçš„å› å­å€¼
        """
        if len(factor_values) == 0:
            return factor_values
        
        lower_bound = factor_values.quantile(quantile)
        upper_bound = factor_values.quantile(1 - quantile)
        
        return factor_values.clip(lower=lower_bound, upper=upper_bound)
    
    def process_factor(self, factor_data: pd.DataFrame, factor_name: str) -> pd.DataFrame:
        """
        å®Œæ•´çš„å› å­å¤„ç†æµç¨‹ï¼šæ–¹å‘è°ƒæ•´ã€ç¼©å°¾ã€ä¸­æ€§åŒ–ã€æ ‡å‡†åŒ–
        
        Args:
            factor_data: åŸå§‹å› å­æ•°æ®
            factor_name: å› å­åç§°
            
        Returns:
            å¤„ç†åçš„å› å­æ•°æ®
        """
        print(f"å¼€å§‹å¤„ç†å› å­: {factor_name}")
        
        processed_factor = factor_data.copy()
        
        # å› å­æ–¹å‘è°ƒæ•´ï¼ˆæ³¢åŠ¨ç‡å› å­é€šå¸¸æ˜¯åå‘å› å­ï¼Œå³æ³¢åŠ¨ç‡è¶Šé«˜æ”¶ç›Šè¶Šä½ï¼‰
        processed_factor = -processed_factor
        
        # é€æ—¥å¤„ç†
        for date in processed_factor.index:
            if date in self.rebalance_dates:
                factor_values = processed_factor.loc[date].dropna()
                
                if len(factor_values) == 0:
                    continue
                
                # 1. ç¼©å°¾å¤„ç†
                factor_values = self._winsorize_factor(factor_values)
                
                # 2. å¸‚å€¼è¡Œä¸šä¸­æ€§åŒ–
                factor_values = self._neutralize_factor(factor_values, date)
                
                # 3. æ ‡å‡†åŒ–
                factor_values = self._standardize_factor(factor_values)
                
                # æ›´æ–°å¤„ç†åçš„å€¼
                processed_factor.loc[date, factor_values.index] = factor_values
        
        print(f"å› å­ {factor_name} å¤„ç†å®Œæˆ")
        return processed_factor
    
    def calculate_factor_ic(self, factor_data: pd.DataFrame) -> Tuple[pd.Series, pd.Series]:
        """
        è®¡ç®—å› å­ICå’ŒRankIC
        
        Args:
            factor_data: å› å­æ•°æ®
            
        Returns:
            (ICåºåˆ—, RankICåºåˆ—)
        """
        ic_series = pd.Series(dtype=float)
        rank_ic_series = pd.Series(dtype=float)
          # è®¡ç®—ä¸‹æœŸæ”¶ç›Šç‡
        next_period_returns = self.data['close'].pct_change().shift(-1)
        
        for i, date in enumerate(self.rebalance_dates[:-1]):  # æ’é™¤æœ€åä¸€ä¸ªæ—¥æœŸ
            next_date = self.rebalance_dates[i + 1]
            
            if date not in factor_data.index or next_date not in next_period_returns.index:
                continue
            
            # è·å–å½“æœŸå› å­å€¼
            factor_values = factor_data.loc[date].dropna()
            
            # ä½¿ç”¨æ›´ç®€å•çš„æœ‰æ•ˆè‚¡ç¥¨è¿‡æ»¤
            if date in self.data['close'].index:
                close_prices = self.data['close'].loc[date]
                valid_stocks = close_prices.dropna().index
                factor_values = factor_values[factor_values.index.intersection(valid_stocks)]
            
            if len(factor_values) < 10:
                continue
            
            # è·å–ä¸‹æœŸæ”¶ç›Šç‡
            returns = next_period_returns.loc[next_date]
            
            # æ‰¾åˆ°å…±åŒè‚¡ç¥¨
            common_stocks = factor_values.index.intersection(returns.dropna().index)
            if len(common_stocks) < 10:
                continue
            
            factor_vals = factor_values[common_stocks]
            return_vals = returns[common_stocks]
            
            try:
                # è®¡ç®—IC (Pearsonç›¸å…³ç³»æ•°)
                ic_val, _ = pearsonr(factor_vals, return_vals)
                if not np.isnan(ic_val):
                    ic_series.loc[date] = ic_val
                  # è®¡ç®—RankIC (Spearmanç›¸å…³ç³»æ•°)
                rank_ic_val, _ = spearmanr(factor_vals, return_vals)
                if not np.isnan(rank_ic_val):
                    rank_ic_series.loc[date] = rank_ic_val
                    
            except Exception as e:
                print(f"è®¡ç®—ICæ—¶å‡ºé”™ (æ—¥æœŸ: {date}): {e}")
                continue
        
        return ic_series, rank_ic_series
    
    def calculate_portfolio_returns(self, factor_data: pd.DataFrame, n_groups: int = 5) -> pd.DataFrame:
        """
        è®¡ç®—åˆ†ç»„ç»„åˆæ”¶ç›Šç‡
        
        Args:
            factor_data: å› å­æ•°æ®
            n_groups: åˆ†ç»„æ•°é‡ï¼Œé»˜è®¤5ç»„
              Returns:
            å„ç»„åˆæ”¶ç›Šç‡DataFrame
        """
        group_returns = pd.DataFrame(index=self.rebalance_dates[:-1])
        
        for i in range(n_groups):
            group_returns[f'ç¬¬{i+1}ç»„'] = 0.0
        
        
        for i, date in enumerate(self.rebalance_dates[:-1]):
            next_date = self.rebalance_dates[i + 1]
            
            if date not in factor_data.index:
                print(f"DEBUG: æ—¥æœŸ {date} ä¸åœ¨å› å­æ•°æ®ä¸­ï¼Œè·³è¿‡")
                continue
            
            # è·å–å½“æœŸå› å­å€¼å¹¶è¿‡æ»¤
            factor_values = factor_data.loc[date].dropna()
            
            # ä½¿ç”¨æ›´ç®€å•çš„æœ‰æ•ˆè‚¡ç¥¨è¿‡æ»¤
            if date in self.data['close'].index:
                close_prices = self.data['close'].loc[date]
                valid_stocks = close_prices.dropna().index
                factor_values = factor_values[factor_values.index.intersection(valid_stocks)]
            
            
            if len(factor_values) < n_groups * 2:  # é™ä½é—¨æ§›ï¼Œæ¯ç»„è‡³å°‘2åªè‚¡ç¥¨
                print(f"DEBUG: è‚¡ç¥¨æ•°é‡ä¸è¶³ ({len(factor_values)} < {n_groups * 2})ï¼Œè·³è¿‡")
                continue
              # åˆ†ç»„ï¼šæŒ‰20%åˆ†ä½æ•°åˆ†ç»„
            factor_values_sorted = factor_values.sort_values()
            n_stocks = len(factor_values_sorted)
              # è®¡ç®—å„ç»„åœ¨ä¸‹ä¸€æœŸçš„æ”¶ç›Šç‡
            period_returns = self._calculate_period_returns(date, next_date, factor_values_sorted.index)
            
            
            if period_returns is None:
                print(f"DEBUG: period_returns ä¸º Noneï¼Œè·³è¿‡")
                continue
            
            
            # æŒ‰20%åˆ†ä½æ•°åˆ†ç»„
            for group_idx in range(n_groups):
                # è®¡ç®—æ¯ç»„çš„èµ·å§‹å’Œç»“æŸä½ç½®
                start_pct = group_idx * 0.2
                end_pct = (group_idx + 1) * 0.2
                
                start_idx = int(start_pct * n_stocks)
                end_idx = int(end_pct * n_stocks)
                
                # æœ€åä¸€ç»„åŒ…å«å‰©ä½™è‚¡ç¥¨
                if group_idx == n_groups - 1:
                    end_idx = n_stocks
                  # ç¡®ä¿æ¯ç»„è‡³å°‘æœ‰1åªè‚¡ç¥¨
                if start_idx >= end_idx:
                    print(f"DEBUG: ç¬¬{group_idx+1}ç»„ç´¢å¼•èŒƒå›´æ— æ•ˆ ({start_idx} >= {end_idx})ï¼Œè·³è¿‡")
                    continue
                    
                group_stocks = factor_values_sorted.iloc[start_idx:end_idx].index
                
                # è®¡ç®—è¯¥ç»„ç­‰æƒé‡æ”¶ç›Šç‡
                if len(group_stocks) > 0:
                    group_return = period_returns[group_stocks].mean()
                    
                    # æ£€æŸ¥èµ‹å€¼æ˜¯å¦æˆåŠŸ
                    column_name = f'ç¬¬{group_idx+1}ç»„'
                    
                    group_returns.loc[date, column_name] = group_return
                    
                else:
                    print(f"DEBUG: ç¬¬{group_idx+1}ç»„æ²¡æœ‰è‚¡ç¥¨")
        
        print(f"DEBUG: è®¡ç®—å®Œæˆågroup_returnsæ ·æœ¬:")
        print(group_returns.head())
        print(f"DEBUG: group_returnsç»Ÿè®¡:")
        print(group_returns.describe())
        
        return group_returns

    def _calculate_period_returns(self, start_date: pd.Timestamp, end_date: pd.Timestamp,
                                   stocks: pd.Index) -> Optional[pd.Series]:
        """
        è®¡ç®—æŒ‡å®šæœŸé—´çš„è‚¡ç¥¨æ”¶ç›Šç‡

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            stocks: è‚¡ç¥¨åˆ—è¡¨
            
        Returns:
            æ”¶ç›Šç‡åºåˆ—
        """
        try:
            
            if start_date not in self.data['close'].index or end_date not in self.data['close'].index:
                print(f"æ—¥æœŸä¸åœ¨ç´¢å¼•ä¸­: start={start_date}, end={end_date}")
                return None
            
            # ç¡®ä¿stocksä¸­çš„è‚¡ç¥¨åœ¨closeæ•°æ®ä¸­å­˜åœ¨
            available_stocks = [stock for stock in stocks if stock in self.data['close'].columns]
            
            if len(available_stocks) == 0:
                print(f"æ²¡æœ‰å¯ç”¨è‚¡ç¥¨åœ¨æ—¥æœŸ {start_date} åˆ° {end_date}")
                return None
                
            start_prices = self.data['close'].loc[start_date, available_stocks]
            end_prices = self.data['close'].loc[end_date, available_stocks]
            
            
            # åªä¿ç•™ä¸¤ä¸ªæ—¥æœŸéƒ½æœ‰ä»·æ ¼çš„è‚¡ç¥¨
            valid_stocks = start_prices.dropna().index.intersection(end_prices.dropna().index)
            
            if len(valid_stocks) == 0:
                print(f"æ²¡æœ‰è‚¡ç¥¨åœ¨ {start_date} åˆ° {end_date} æœŸé—´æœ‰å®Œæ•´ä»·æ ¼æ•°æ®")
                return None
            
            start_prices = start_prices[valid_stocks]
            end_prices = end_prices[valid_stocks]
            
            
            # è®¡ç®—æ”¶ç›Šç‡
            returns = (end_prices / start_prices - 1)
            
            
            if len(returns) == 0:
                print(f"è®¡ç®—å‡ºçš„æ”¶ç›Šç‡åºåˆ—ä¸ºç©ºï¼Œæ—¥æœŸ: {start_date} åˆ° {end_date}")
                return None
                
            return returns
            
        except Exception as e:
            print(f"è®¡ç®—æœŸé—´æ”¶ç›Šç‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def calculate_benchmark_returns(self) -> pd.Series:
        """
        è®¡ç®—åŸºå‡†æ”¶ç›Šç‡ï¼ˆä½¿ç”¨CSI300æŒ‡æ•°ï¼‰
        
        Returns:
            åŸºå‡†æ”¶ç›Šç‡åºåˆ—
        """
        benchmark_returns = pd.Series(index=self.rebalance_dates[:-1], dtype=float)
        
        if 'csi300_index_returns' not in self.data:
            print("è­¦å‘Šï¼šæ— CSI300æŒ‡æ•°æ•°æ®ï¼Œä½¿ç”¨å¸‚åœºå¹³å‡æ”¶ç›Šä½œä¸ºåŸºå‡†")
            # ä½¿ç”¨å…¨å¸‚åœºå¹³å‡æ”¶ç›Šä½œä¸ºæ›¿ä»£
            for i, date in enumerate(self.rebalance_dates[:-1]):
                next_date = self.rebalance_dates[i + 1]
                period_returns = self._calculate_period_returns(date, next_date, self.data['close'].columns)
                if period_returns is not None:
                    benchmark_returns.loc[date] = period_returns.mean()
            return benchmark_returns
        
        # ä½¿ç”¨CSI300æŒ‡æ•°æ”¶ç›Šç‡
        for i, date in enumerate(self.rebalance_dates[:-1]):
            next_date = self.rebalance_dates[i + 1]
              # è®¡ç®—CSI300åœ¨è¯¥æœŸé—´çš„æ”¶ç›Šç‡
            csi300_data = self.data['csi300_index_returns']
            period_data = csi300_data[(csi300_data.index > date) & (csi300_data.index <= next_date)]
            
            if len(period_data) > 0:
                # ç´¯è®¡æ”¶ç›Šç‡
                cumulative_return = (1 + period_data).prod() - 1
                benchmark_returns.loc[date] = cumulative_return
        
        return benchmark_returns
    
    def run_single_factor_backtest(self, factor_name: str) -> Dict:
        """
        å¯¹å•ä¸ªå› å­è¿›è¡Œå›æµ‹
        
        Args:
            factor_name: å› å­åç§°
            
        Returns:
            å›æµ‹ç»“æœå­—å…¸
        """
        print(f"\nå¼€å§‹å›æµ‹å› å­: {factor_name}")
        
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        if not hasattr(self, 'data') or 'close' not in self.data:
            print("æ•°æ®æœªåŠ è½½ï¼Œå¼€å§‹åŠ è½½æ•°æ®...")
            if not self.load_cached_data():
                raise RuntimeError("æ•°æ®åŠ è½½å¤±è´¥")
            self._get_month_end_dates()
        
        # è®¡ç®—å› å­
        if factor_name not in self.factors_config:
            raise ValueError(f"æœªçŸ¥å› å­: {factor_name}")
        
        factor_func = self.factors_config[factor_name]['func']
        factor_params = self.factors_config[factor_name]['params']
        print(f"è®¡ç®— {factor_name} å› å­...")
        # è·å–CSI300è¿‡æ»¤åçš„ä»·æ ¼æ•°æ®è¿›è¡Œå› å­è®¡ç®—
        csi300_close_data = self._get_csi300_filtered_data()
        raw_factor_data = factor_func(csi300_close_data, **factor_params)
        
        # å¤„ç†å› å­
        processed_factor_data = self.process_factor(raw_factor_data, factor_name)
        
        # è®¡ç®—IC
        print(f"è®¡ç®— {factor_name} ICå€¼...")
        ic_series, rank_ic_series = self.calculate_factor_ic(processed_factor_data)
        
        # è®¡ç®—åˆ†ç»„æ”¶ç›Šç‡
        print(f"è®¡ç®— {factor_name} åˆ†ç»„æ”¶ç›Šç‡...")
        group_returns = self.calculate_portfolio_returns(processed_factor_data)
        
        # è®¡ç®—å¤šç©ºç»„åˆæ”¶ç›Šç‡
        if len(group_returns.columns) >= 2:
            long_short_returns = group_returns['ç¬¬1ç»„'] - group_returns['ç¬¬5ç»„']
        else:
            long_short_returns = pd.Series(dtype=float)
        
        result = {
            'ICå€¼åºåˆ—': ic_series,
            'RankICå€¼åºåˆ—': rank_ic_series,
            'å„åˆ†ç»„æ”¶ç›Šç‡': group_returns,
            'å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—': long_short_returns
        }
        
        print(f"{factor_name} å›æµ‹å®Œæˆ")
        
        # æ›´æ–°æ—¥å¿—
        self._update_log_with_factor(factor_name, result)
        
        return result
    
    def run_backtest(self, factors: Optional[List[str]] = None) -> Dict:
        """
        è¿è¡Œå®Œæ•´å›æµ‹
        
        Args:
            factors: è¦å›æµ‹çš„å› å­åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å›æµ‹æ‰€æœ‰å› å­
            
        Returns:
            å®Œæ•´å›æµ‹ç»“æœ
        """
        print("å¼€å§‹æ‰§è¡Œå› å­å›æµ‹...")
        
        # åŠ è½½æ•°æ®
        if not self.load_cached_data():
            raise RuntimeError("æ•°æ®åŠ è½½å¤±è´¥")
        
        # è·å–è°ƒä»“æ—¥æœŸ
        self._get_month_end_dates()
        
        if len(self.rebalance_dates) < 2:
            raise RuntimeError("è°ƒä»“æ—¥æœŸä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå›æµ‹")
        
        # ç¡®å®šè¦å›æµ‹çš„å› å­
        if factors is None:
            factors = list(self.factors_config.keys())
        
        # è®¡ç®—åŸºå‡†æ”¶ç›Šç‡
        print("è®¡ç®—åŸºå‡†æ”¶ç›Šç‡...")
        benchmark_returns = self.calculate_benchmark_returns()
        
        # æ›´æ–°æ—¥å¿—ï¼šåŸºå‡†æ•°æ®
        self._update_log_with_benchmark(self.rebalance_dates[:-1], benchmark_returns.tolist())
        
        # å›æµ‹å„å› å­
        factor_results = {}
        for factor_name in factors:
            try:
                factor_results[factor_name] = self.run_single_factor_backtest(factor_name)
            except Exception as e:
                print(f"å›æµ‹å› å­ {factor_name} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ•´ç†ç»“æœ
        result = {
            "å›æµ‹æ—¥æœŸåºåˆ—": self.rebalance_dates[:-1],
            "åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—": benchmark_returns,
            "å› å­è¡¨ç°æ•°æ®": factor_results
        }
        
        print("\nå›æµ‹å®Œæˆï¼")
        self._print_backtest_summary(result)
        
        # å®Œæˆæ—¥å¿—        self._finalize_log(len(factor_results))
        
        return result
    
    def _print_results_in_required_format(self, results: Dict, save_to_file: bool = True):
        """
        æŒ‰è¦æ±‚æ ¼å¼æ‰“å°å›æµ‹ç»“æœ
        
        Args:
            results: å›æµ‹ç»“æœå­—å…¸
            save_to_file: æ˜¯å¦ä¿å­˜åˆ°log.logæ–‡ä»¶ï¼Œé»˜è®¤True
        """
        print("\n" + "="*80)
        print("å›æµ‹ç»“æœ - æŒ‰è¦æ±‚æ ¼å¼è¾“å‡º")
        print("="*80)
        
        # å‡†å¤‡è¾“å‡ºå­—å…¸
        output_dict = {
            "å›æµ‹æ—¥æœŸåºåˆ—": [date.strftime('%Y-%m-%d') for date in results["å›æµ‹æ—¥æœŸåºåˆ—"]],
            "åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—": results["åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—"],
            "å› å­è¡¨ç°æ•°æ®": {}
        }
        
        # å¤„ç†å› å­æ•°æ®
        for factor_name, factor_data in results["å› å­è¡¨ç°æ•°æ®"].items():
            output_dict["å› å­è¡¨ç°æ•°æ®"][factor_name] = {
                "ICå€¼åºåˆ—": factor_data["ICå€¼åºåˆ—"],
                "RankICå€¼åºåˆ—": factor_data["RankICå€¼åºåˆ—"],
                "å„åˆ†ç»„æ”¶ç›Šç‡": {
                    "ç¬¬ä¸€ç»„": factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"]["ç¬¬1ç»„"].values.tolist() if "ç¬¬1ç»„" in factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"].columns else [],
                    "ç¬¬äºŒç»„": factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"]["ç¬¬2ç»„"].values.tolist() if "ç¬¬2ç»„" in factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"].columns else [],
                    "ç¬¬ä¸‰ç»„": factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"]["ç¬¬3ç»„"].values.tolist() if "ç¬¬3ç»„" in factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"].columns else [],
                    "ç¬¬å››ç»„": factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"]["ç¬¬4ç»„"].values.tolist() if "ç¬¬4ç»„" in factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"].columns else [],
                    "ç¬¬äº”ç»„": factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"]["ç¬¬5ç»„"].values.tolist() if "ç¬¬5ç»„" in factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"].columns else []
                },
                "å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—": factor_data["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"]
            }
        
        # æ‰“å°ç»“æœ
        import json
        print(json.dumps(output_dict, indent=2, ensure_ascii=False))
        print("="*80)
        
        # æ‰“å°æ•°æ®ç»“æ„ä¿¡æ¯
        print("\næ•°æ®ç»“æ„ä¿¡æ¯:")
        print(f"å›æµ‹æ—¥æœŸåºåˆ—é•¿åº¦: {len(output_dict['å›æµ‹æ—¥æœŸåºåˆ—'])}")
        print(f"åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—é•¿åº¦: {len(output_dict['åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—'])}")
        print(f"å›æµ‹å› å­æ•°é‡: {len(output_dict['å› å­è¡¨ç°æ•°æ®'])}")
        for factor_name, factor_data in output_dict["å› å­è¡¨ç°æ•°æ®"].items():
            print(f"\n{factor_name}:")
            print(f"  - ICå€¼åºåˆ—é•¿åº¦: {len(factor_data['ICå€¼åºåˆ—'])}")
            print(f"  - RankICå€¼åºåˆ—é•¿åº¦: {len(factor_data['RankICå€¼åºåˆ—'])}")
            print(f"  - å„åˆ†ç»„æ”¶ç›Šç‡æ•°æ®ç‚¹æ•°: {len(factor_data['å„åˆ†ç»„æ”¶ç›Šç‡']['ç¬¬ä¸€ç»„'])}")
            print(f"  - å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—é•¿åº¦: {len(factor_data['å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—'])}")
        
        print("="*80)
        
        # ä¿å­˜ç»“æœåˆ°log.logæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if save_to_file:
            try:
                # è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•è·¯å¾„
                current_dir = os.path.dirname(__file__)
                log_file_path = os.path.join(current_dir, 'log.log')
                
                # æ·»åŠ æ—¶é—´æˆ³ä¿¡æ¯
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                log_data = {
                    "æ—¶é—´æˆ³": timestamp,
                    "å›æµ‹ç»“æœ": output_dict
                }
                  # ä¿å­˜ä¸ºJSONæ ¼å¼
                with open(log_file_path, 'w', encoding='utf-8') as f:
                    json.dump(log_data, f, indent=2, ensure_ascii=False)
                
                print(f"\nâœ… å›æµ‹ç»“æœå·²ä¿å­˜åˆ°: {log_file_path}")
                
            except Exception as e:
                print(f"\nâŒ ä¿å­˜log.logæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶å·²é€šè¿‡å¢é‡æ›´æ–°å®Œæˆï¼Œè·³è¿‡æ‰¹é‡ä¿å­˜")

    def run_comprehensive_backtest(self, use_incremental_logging: bool = True) -> Dict:
        """
        è¿è¡Œç»¼åˆå›æµ‹ï¼Œå¯¹æ‰€æœ‰å¯ç”¨å› å­è¿›è¡Œå›æµ‹åˆ†æ
        
        Args:
            use_incremental_logging: æ˜¯å¦ä½¿ç”¨å¢é‡æ—¥å¿—è®°å½•ï¼Œé»˜è®¤True
        
        Returns:
            ç¬¦åˆè¦æ±‚æ ¼å¼çš„å›æµ‹ç»“æœå­—å…¸
        """
        print("å¼€å§‹ç»¼åˆå›æµ‹...")
        
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        if not hasattr(self, 'data') or 'close' not in self.data:
            print("æ•°æ®æœªåŠ è½½ï¼Œå¼€å§‹åŠ è½½æ•°æ®...")
            if not self.load_cached_data():
                raise RuntimeError("æ•°æ®åŠ è½½å¤±è´¥")
            self._get_month_end_dates()
        
        # 1. åˆå§‹åŒ–å¢é‡æ—¥å¿—
        if use_incremental_logging:
            print("åˆå§‹åŒ–å¢é‡æ—¥å¿—ç³»ç»Ÿ...")
            self._initialize_log()
        
        # 2. è®¡ç®—åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡
        print("è®¡ç®—åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡...")
        benchmark_returns = self.calculate_benchmark_returns()
        
        # 3. ç«‹å³æ›´æ–°æ—¥å¿—ï¼šå›æµ‹æ—¥æœŸå’ŒåŸºå‡†æ”¶ç›Šç‡
        if use_incremental_logging:
            self._update_log_with_benchmark(
                self.rebalance_dates[:-1],  # æ’é™¤æœ€åä¸€ä¸ªæ—¥æœŸ
                benchmark_returns.values.tolist()
            )
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        results = {
            "å›æµ‹æ—¥æœŸåºåˆ—": self.rebalance_dates[:-1],  # æ’é™¤æœ€åä¸€ä¸ªæ—¥æœŸ
            "åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—": benchmark_returns.values.tolist(),
            "å› å­è¡¨ç°æ•°æ®": {}
        }
        
        # 4. å¯¹æ¯ä¸ªå› å­è¿›è¡Œå›æµ‹ï¼Œå¹¶å®æ—¶æ›´æ–°æ—¥å¿—
        completed_factors = 0
        for factor_name in self.factors_config.keys():
            print(f"\n{'='*50}")
            print(f"å¼€å§‹å›æµ‹å› å­: {factor_name}")
            print(f"{'='*50}")
            
            try:
                factor_results = self.run_single_factor_backtest(factor_name)
                
                # å°†ç»“æœæ ¼å¼åŒ–ä¸ºè¦æ±‚çš„æ ¼å¼
                results["å› å­è¡¨ç°æ•°æ®"][factor_name] = {
                    "ICå€¼åºåˆ—": factor_results["ICå€¼åºåˆ—"].values.tolist(),
                    "RankICå€¼åºåˆ—": factor_results["RankICå€¼åºåˆ—"].values.tolist(),
                    "å„åˆ†ç»„æ”¶ç›Šç‡": factor_results["å„åˆ†ç»„æ”¶ç›Šç‡"],  # ä¿æŒDataFrameæ ¼å¼ç”¨äºåç»­å¤„ç†
                    "å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—": factor_results["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"].values.tolist()
                }
                
                # ç«‹å³æ›´æ–°æ—¥å¿—ï¼šæ·»åŠ å› å­ç»“æœ
                if use_incremental_logging:
                    self._update_log_with_factor(factor_name, factor_results)
                
                completed_factors += 1
                print(f"å› å­ {factor_name} å›æµ‹å®Œæˆ ({completed_factors}/{len(self.factors_config)})")
                print(f"- ICå‡å€¼: {factor_results['ICå€¼åºåˆ—'].mean():.4f}")
                print(f"- ICæ ‡å‡†å·®: {factor_results['ICå€¼åºåˆ—'].std():.4f}")
                if factor_results['ICå€¼åºåˆ—'].std() != 0:
                    print(f"- IC_IR: {factor_results['ICå€¼åºåˆ—'].mean() / factor_results['ICå€¼åºåˆ—'].std():.4f}")
                print(f"- RankICå‡å€¼: {factor_results['RankICå€¼åºåˆ—'].mean():.4f}")
                print(f"- å¤šç©ºç»„åˆå¹´åŒ–æ”¶ç›Šç‡: {(factor_results['å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—'].mean() * 12):.4f}")
                
            except Exception as e:
                print(f"å› å­ {factor_name} å›æµ‹å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # 5. å®Œæˆå¢é‡æ—¥å¿—
        if use_incremental_logging:
            self._finalize_log(completed_factors)
        
        print(f"\n{'='*50}")
        print("ç»¼åˆå›æµ‹å®Œæˆ!")
        print(f"æˆåŠŸå›æµ‹å› å­æ•°é‡: {completed_factors}")
        print(f"å›æµ‹æœŸé—´: {self.start_date.strftime('%Y-%m-%d')} åˆ° {self.end_date.strftime('%Y-%m-%d')}")
        print(f"è°ƒä»“å‘¨æœŸæ•°: {len(results['å›æµ‹æ—¥æœŸåºåˆ—'])}")
        print(f"{'='*50}")
        
        # 6. è¾“å‡ºç»“æœåˆ°æ§åˆ¶å°ï¼ˆå¯é€‰æ‹©æ˜¯å¦ä¿å­˜æ–‡ä»¶ï¼‰
        self._print_results_in_required_format(results, save_to_file=not use_incremental_logging)
        
        return results
    
    def _print_backtest_summary(self, result: Dict):
        """æ‰“å°å›æµ‹ç»“æœæ‘˜è¦"""
        print("\n=== å›æµ‹ç»“æœæ‘˜è¦ ===")
        print(f"å›æµ‹æœŸé—´: {self.start_date.strftime('%Y-%m-%d')} åˆ° {self.end_date.strftime('%Y-%m-%d')}")
        print(f"è°ƒä»“æ¬¡æ•°: {len(result['å›æµ‹æ—¥æœŸåºåˆ—'])}")
        
        for factor_name, factor_data in result["å› å­è¡¨ç°æ•°æ®"].items():
            print(f"\n--- {factor_name} ---")
            
            # ICç»Ÿè®¡
            ic_mean = factor_data['ICå€¼åºåˆ—'].mean()
            ic_std = factor_data['ICå€¼åºåˆ—'].std()
            ic_ir = ic_mean / ic_std if ic_std != 0 else 0
            
            rank_ic_mean = factor_data['RankICå€¼åºåˆ—'].mean()
            rank_ic_std = factor_data['RankICå€¼åºåˆ—'].std()
            rank_ic_ir = rank_ic_mean / rank_ic_std if rank_ic_std != 0 else 0
            
            print(f"ICå‡å€¼: {ic_mean:.4f}, ICæ ‡å‡†å·®: {ic_std:.4f}, ICIR: {ic_ir:.4f}")
            print(f"RankICå‡å€¼: {rank_ic_mean:.4f}, RankICæ ‡å‡†å·®: {rank_ic_std:.4f}, RankICIR: {rank_ic_ir:.4f}")
            
            # å¤šç©ºç»„åˆç»Ÿè®¡
            if len(factor_data['å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—']) > 0:
                ls_mean = factor_data['å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—'].mean()
                ls_std = factor_data['å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—'].std()
                ls_sharpe = ls_mean / ls_std if ls_std != 0 else 0
                
                print(f"å¤šç©ºç»„åˆå¹´åŒ–æ”¶ç›Š: {ls_mean * 12:.2%}")
                print(f"å¤šç©ºç»„åˆå¹´åŒ–æ³¢åŠ¨: {ls_std * np.sqrt(12):.2%}")
                print(f"å¤šç©ºç»„åˆSharpe: {ls_sharpe:.4f}")
    
    def save_results(self, results: Dict, filename: str = None):
        """
        ä¿å­˜å›æµ‹ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: å›æµ‹ç»“æœå­—å…¸
            filename: ä¿å­˜æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if filename is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"backtesting_results_{timestamp}.pkl"
        
        # ç¡®ä¿Resultç›®å½•å­˜åœ¨
        result_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'Result')
        os.makedirs(result_dir, exist_ok=True)
        
        filepath = os.path.join(result_dir, filename)
        
        with open(filepath, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"å›æµ‹ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def _get_log_file_path(self) -> str:
        """è·å–log.logæ–‡ä»¶è·¯å¾„"""
        current_dir = os.path.dirname(__file__)
        return os.path.join(current_dir, 'log.log')
    def _initialize_log(self) -> None:
        """
        åˆå§‹åŒ–log.logæ–‡ä»¶ï¼Œè®¾ç½®åŸºæœ¬ç»“æ„
        """
        try:
            log_file_path = self._get_log_file_path()
            
            # åˆå§‹åŒ–logæ•°æ®ç»“æ„ - ä¸¥æ ¼æŒ‰ç…§è¦æ±‚æ ¼å¼
            log_data = {
                "å›æµ‹æ—¥æœŸåºåˆ—": [],
                "åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—": [],
                "å› å­è¡¨ç°æ•°æ®": {}
            }
            
            # ä¿å­˜åˆå§‹ç»“æ„
            import json
            with open(log_file_path, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å·²åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶: {log_file_path}")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–log.logæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _update_log_with_benchmark(self, backtest_dates: list, benchmark_returns: list) -> None:
        """
        æ›´æ–°log.logæ–‡ä»¶ï¼Œæ·»åŠ å›æµ‹æ—¥æœŸå’ŒåŸºå‡†æ”¶ç›Šç‡
        
        Args:
            backtest_dates: å›æµ‹æ—¥æœŸåºåˆ—
            benchmark_returns: åŸºå‡†æ”¶ç›Šç‡åºåˆ—
        """
        try:
            log_file_path = self._get_log_file_path()
            import os
            if not os.path.exists(log_file_path):
                self._initialize_log()
            # è¯»å–ç°æœ‰logæ•°æ®
            import json
            with open(log_file_path, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            # æ›´æ–°å›æµ‹æ—¥æœŸå’ŒåŸºå‡†æ”¶ç›Šç‡ï¼ˆå¤„ç†å­—ç¬¦ä¸²å’Œdatetimeå¯¹è±¡ï¼‰
            if backtest_dates and isinstance(backtest_dates[0], str):
                log_data["å›æµ‹æ—¥æœŸåºåˆ—"] = backtest_dates
            else:
                log_data["å›æµ‹æ—¥æœŸåºåˆ—"] = [date.strftime('%Y-%m-%d') for date in backtest_dates]
            log_data["åŸºå‡†æŒ‡æ•°æ”¶ç›Šç‡åºåˆ—"] = benchmark_returns
            
            # ä¿å­˜æ›´æ–°çš„æ•°æ®
            with open(log_file_path, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å·²æ›´æ–°æ—¥å¿—ï¼šå›æµ‹æ—¥æœŸ({len(backtest_dates)}ä¸ª) å’Œ åŸºå‡†æ”¶ç›Šç‡({len(benchmark_returns)}ä¸ª)")
            
        except Exception as e:
            print(f"âŒ æ›´æ–°log.log(åŸºå‡†æ•°æ®)æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _update_log_with_factor(self, factor_name: str, factor_results: Dict) -> None:
        """
        æ›´æ–°log.logæ–‡ä»¶ï¼Œæ·»åŠ å•ä¸ªå› å­çš„å›æµ‹ç»“æœ
        
        Args:
            factor_name: å› å­åç§°
            factor_results: å› å­å›æµ‹ç»“æœ
        """
        try:
            log_file_path = self._get_log_file_path()
            import os
            if not os.path.exists(log_file_path):
                self._initialize_log()
            # è¯»å–ç°æœ‰logæ•°æ®
            import json
            with open(log_file_path, 'r', encoding='utf-8') as f:
                log_data = json.load(f)            # æ ¼å¼åŒ–å› å­æ•°æ®ä¸ºè¦æ±‚çš„æ ¼å¼ï¼ˆå¤„ç†ä¸åŒçš„è¾“å…¥æ ¼å¼ï¼‰
            formatted_factor_data = {}
            
            # å¤„ç†ICå€¼åºåˆ—
            if "ICå€¼åºåˆ—" in factor_results and hasattr(factor_results["ICå€¼åºåˆ—"], 'values'):
                formatted_factor_data["ICå€¼åºåˆ—"] = factor_results["ICå€¼åºåˆ—"].values.tolist()
            elif "ICå€¼åºåˆ—" in factor_results:
                formatted_factor_data["ICå€¼åºåˆ—"] = factor_results["ICå€¼åºåˆ—"]
            else:
                formatted_factor_data["ICå€¼åºåˆ—"] = []
            
            # å¤„ç†RankICå€¼åºåˆ—
            if "RankICå€¼åºåˆ—" in factor_results and hasattr(factor_results["RankICå€¼åºåˆ—"], 'values'):
                formatted_factor_data["RankICå€¼åºåˆ—"] = factor_results["RankICå€¼åºåˆ—"].values.tolist()
            elif "RankICå€¼åºåˆ—" in factor_results:
                formatted_factor_data["RankICå€¼åºåˆ—"] = factor_results["RankICå€¼åºåˆ—"]
            else:
                formatted_factor_data["RankICå€¼åºåˆ—"] = []
            
            # å¤„ç†å„åˆ†ç»„æ”¶ç›Šç‡
            if "å„åˆ†ç»„æ”¶ç›Šç‡" in factor_results:
                group_returns = factor_results["å„åˆ†ç»„æ”¶ç›Šç‡"]
                if hasattr(group_returns, 'columns'):  # DataFrame
                    formatted_factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"] = {
                        "ç¬¬ä¸€ç»„": group_returns["ç¬¬1ç»„"].values.tolist() if "ç¬¬1ç»„" in group_returns.columns else [],
                        "ç¬¬äºŒç»„": group_returns["ç¬¬2ç»„"].values.tolist() if "ç¬¬2ç»„" in group_returns.columns else [],
                        "ç¬¬ä¸‰ç»„": group_returns["ç¬¬3ç»„"].values.tolist() if "ç¬¬3ç»„" in group_returns.columns else [],
                        "ç¬¬å››ç»„": group_returns["ç¬¬4ç»„"].values.tolist() if "ç¬¬4ç»„" in group_returns.columns else [],
                        "ç¬¬äº”ç»„": group_returns["ç¬¬5ç»„"].values.tolist() if "ç¬¬5ç»„" in group_returns.columns else []
                    }
                else:  # Dict
                    formatted_factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"] = group_returns
            else:
                formatted_factor_data["å„åˆ†ç»„æ”¶ç›Šç‡"] = {}
            
            # å¤„ç†å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—
            if "å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—" in factor_results and hasattr(factor_results["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"], 'values'):
                formatted_factor_data["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"] = factor_results["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"].values.tolist()
            elif "å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—" in factor_results:
                formatted_factor_data["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"] = factor_results["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"]
            else:
                formatted_factor_data["å¤šç©ºç»„åˆæ”¶ç›Šç‡åºåˆ—"] = []
            
            # æ·»åŠ å› å­æ•°æ®åˆ°log - ä¸¥æ ¼æŒ‰ç…§è¦æ±‚æ ¼å¼
            log_data["å› å­è¡¨ç°æ•°æ®"][factor_name] = formatted_factor_data
            
            # ä¿å­˜æ›´æ–°çš„æ•°æ®
            with open(log_file_path, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å·²æ›´æ–°æ—¥å¿—ï¼šå› å­ {factor_name} å›æµ‹ç»“æœ")
            
        except Exception as e:
            print(f"âŒ æ›´æ–°log.log(å› å­ {factor_name})æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _finalize_log(self, total_factors_completed: int) -> None:
        """
        å®Œæˆlog.logæ–‡ä»¶ï¼Œæ·»åŠ æœ€ç»ˆæ—¶é—´æˆ³å’Œæ€»ç»“ä¿¡æ¯
        
        Args:
            total_factors_completed: æˆåŠŸå®Œæˆçš„å› å­æ•°é‡
        """
        try:
            log_file_path = self._get_log_file_path()

            # è¯»å–ç°æœ‰logæ•°æ®
            import json
            with open(log_file_path, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            # ä¸æ·»åŠ ä»»ä½•é¢å¤–ä¿¡æ¯ï¼Œä¿æŒä¸¥æ ¼æ ¼å¼
            # log.logæ–‡ä»¶å·²åŒ…å«æ‰€éœ€çš„æ‰€æœ‰æ•°æ®ï¼Œæ— éœ€æ·»åŠ å®Œæˆæ ‡è®°
            
            print(f"âœ… æ—¥å¿—æ–‡ä»¶å·²å®Œæˆï¼Œå…±æˆåŠŸå›æµ‹ {total_factors_completed} ä¸ªå› å­")
            
        except Exception as e:
            print(f"âŒ å®Œæˆlog.logæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _get_csi300_filtered_data(self) -> pd.DataFrame:
        """
        è·å–CSI300æˆåˆ†è‚¡è¿‡æ»¤åçš„ä»·æ ¼æ•°æ®
        åªä¿ç•™åœ¨å›æµ‹æœŸé—´ä»»æ„æ—¶ç‚¹æ˜¯CSI300æˆåˆ†è‚¡çš„è‚¡ç¥¨
        
        Returns:
            è¿‡æ»¤åçš„æ”¶ç›˜ä»·æ•°æ®
        """
        print("å¼€å§‹è¿‡æ»¤CSI300æˆåˆ†è‚¡æ•°æ®...")
        
        if 'csi300_constituents' not in self.data:
            print("è­¦å‘Šï¼šæ— CSI300æˆåˆ†è‚¡æ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨")
            return self.data['close']
        
        # è·å–åœ¨å›æµ‹æœŸé—´ä»»æ„æ—¶ç‚¹æ˜¯CSI300æˆåˆ†è‚¡çš„æ‰€æœ‰è‚¡ç¥¨
        csi300_constituents = self.data['csi300_constituents']
        
        # æ‰¾åˆ°æ‰€æœ‰æ›¾ç»æ˜¯CSI300æˆåˆ†è‚¡çš„è‚¡ç¥¨
        all_csi300_stocks = set()
        for date in csi300_constituents.index:
            if self.start_date <= date <= self.end_date:
                current_constituents = csi300_constituents.loc[date]
                csi300_stocks = current_constituents[current_constituents == True].index
                all_csi300_stocks.update(csi300_stocks)
        
        all_csi300_stocks = list(all_csi300_stocks)
        
        # è¿‡æ»¤ä»·æ ¼æ•°æ®ï¼Œåªä¿ç•™CSI300ç›¸å…³è‚¡ç¥¨
        close_data = self.data['close']
        available_stocks = [stock for stock in all_csi300_stocks if stock in close_data.columns]
        
        filtered_close = close_data[available_stocks]
        
        print(f"CSI300è‚¡ç¥¨æ± è¿‡æ»¤å®Œæˆ: {len(available_stocks)} åªè‚¡ç¥¨ (åŸæ€»æ•°: {len(close_data.columns)})")
        
        return filtered_close

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºå›æµ‹å™¨
    backtester = FactorBacktester(start_date='2021-01-01', end_date='2025-04-30')
    
    # è¿è¡Œå›æµ‹
    results = backtester.run_backtest()
    
    # ä¿å­˜ç»“æœ
    results_path = os.path.join(os.path.dirname(CURRENT_DIR), 'Result', 'backtest_results.pkl')
    os.makedirs(os.path.dirname(results_path), exist_ok=True)
    
    with open(results_path, 'wb') as f:
        pickle.dump(results, f)
    print(f"\nå›æµ‹ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    return results


if __name__ == "__main__":
    main()